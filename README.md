# AMD_Robotics_Hackathon_2025_LeCoup_De_Pouce

## Team Information

**Team:** *Team 11, IFT, Lucas JOURDET, Fabien PIERETTI, Leo GUERIN, Victor RANGUIN*

**Summary:** *This project's goal was to train the robotic arm so101 to recognize a specific vocal command and to give the object related to that command. The robot had to give a glove, pliers and a syringe.
It was trained using SmolVLA and used a compartiment module to make sure to separate the different objects.*

*< Images or video demonstrating your project >*

## Submission Details

### 1. Mission Description
- *In the medical world, using a robot to hand the different tools to a surgeon appears to be safer because of the lack of mistake the model can make. The idea is to never have any physical unsterilized contact with the object in between the movement from the table to the surgeon's hand.*

### 2. Creativity
- *Using a robotic arm piloted by AI allows for more flexibility and adaptation from the robot to the surgeon's habits. The so101 is also a cheap alternative rather than big complex arms that cost a lot of money.*
- *The innovation resides in the methodology, using an AI model to pilot the arm and in the application, replacing a human assistant in such a specific domain of application.*

### 3. Technical implementations
- *Teleoperation / Dataset capture*
    - [Watch the teleoperation video](/Training_teleoperation.mp4)
- *Training*
- *Inference*
    - *<Image/video of inference eval>*

### 4. Ease of use
- *It can work in any operating room as long as the tools are stored in similar boxes as in the original dataset.*
- *It can adapt to different hand position.*
- *You'll need lerobot framework and the model for vocal command recognition and the python script.*

## Additional Links
*For example, you can provide links to:*

- *Link to a video of your robot performing the task*
- *URL of your dataset in Hugging Face*
- *URL of your model in Hugging Face*
- *Link to a blog post describing your work*

## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
